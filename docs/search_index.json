[["bayesian-data-analysis-3.html", "Chapter 24 Bayesian data analysis 3 24.1 Learning goals 24.2 Load packages and set plotting theme 24.3 Evidence for the null hypothesis 24.4 Dealing with heteroscedasticity 24.5 Zero-one inflated beta binomial model 24.6 Ordinal regression 24.7 Additional resources 24.8 Session info", " Chapter 24 Bayesian data analysis 3 24.1 Learning goals Evidence for null results. Only positive predictors. Dealing with unequal variance. Modeling slider data: Zero-one inflated beta binomial model. Modeling Likert scale data: Ordinal logistic regression. 24.2 Load packages and set plotting theme library(&quot;knitr&quot;) # for knitting RMarkdown library(&quot;kableExtra&quot;) # for making nice tables library(&quot;janitor&quot;) # for cleaning column names library(&quot;tidybayes&quot;) # tidying up results from Bayesian models library(&quot;brms&quot;) # Bayesian regression models with Stan library(&quot;patchwork&quot;) # for making figure panels library(&quot;GGally&quot;) # for pairs plot library(&quot;broom.mixed&quot;) # for tidy lmer results library(&quot;bayesplot&quot;) # for visualization of Bayesian model fits library(&quot;modelr&quot;) # for modeling functions library(&quot;lme4&quot;) # for linear mixed effects models library(&quot;afex&quot;) # for ANOVAs library(&quot;car&quot;) # for ANOVAs library(&quot;emmeans&quot;) # for linear contrasts library(&quot;ggeffects&quot;) # for help with logistic regressions library(&quot;titanic&quot;) # titanic dataset library(&quot;gganimate&quot;) # for animations library(&quot;parameters&quot;) # for getting parameters library(&quot;transformr&quot;) # for gganimate library(&quot;rstanarm&quot;) # for Bayesian models library(&quot;ggrepel&quot;) # for labels in ggplots library(&quot;scales&quot;) # for percent y-axis library(&quot;tidyverse&quot;) # for wrangling, plotting, etc. theme_set(theme_classic() + #set the theme theme(text = element_text(size = 20))) #set the default text size # set rstan options rstan::rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) opts_chunk$set(comment = &quot;&quot;, fig.show = &quot;hold&quot;) 24.3 Evidence for the null hypothesis See this tutorial and this paper (Wagenmakers et al. 2010) for more information. 24.3.1 Bayes factor 24.3.1.1 Fit the model Define a binomial model Give a uniform prior beta(1, 1) Get samples from the prior df.null = tibble(s = 6, k = 10) fit.brm_bayes = brm(s | trials(k) ~ 0 + Intercept, family = binomial(link = &quot;identity&quot;), prior = set_prior(&quot;beta(1, 1)&quot;, class = &quot;b&quot;, lb = 0, ub = 1), data = df.null, sample_prior = TRUE, cores = 4, file = &quot;cache/brm_bayes&quot;) 24.3.1.2 Visualize the results Visualize the prior and posterior samples: fit.brm_bayes %&gt;% as_draws_df(variable = &quot;[b]&quot;, regex = T) %&gt;% pivot_longer(cols = -contains(&quot;.&quot;)) %&gt;% ggplot(mapping = aes(x = value, fill = name)) + geom_density(alpha = 0.5) + scale_fill_brewer(palette = &quot;Set1&quot;) fit.brm_bayes %&gt;% as_draws_df(variable = &quot;[b]&quot;, regex = T) # A draws_df: 1000 iterations, 4 chains, and 2 variables b_Intercept prior_b 1 0.63 0.99 2 0.46 0.77 3 0.52 0.42 4 0.69 1.00 5 0.41 0.82 6 0.47 0.25 7 0.69 0.77 8 0.42 0.72 9 0.68 0.51 10 0.42 0.73 # ... with 3990 more draws # ... hidden reserved variables {&#39;.chain&#39;, &#39;.iteration&#39;, &#39;.draw&#39;} We test the H0: \\(\\theta = 0.5\\) versus the H1: \\(\\theta \\neq 0.5\\) using the Savage-Dickey Method, according to which we can compute the Bayes factor like so: \\(BF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} = \\frac{p(\\theta = 0.5|D, H_1)}{p(\\theta = 0.5|H_1)}\\) fit.brm_bayes %&gt;% hypothesis(hypothesis = &quot;Intercept = 0.5&quot;) Hypothesis Tests for class b: Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio 1 (Intercept)-(0.5) = 0 0.08 0.14 -0.21 0.34 2.13 Post.Prob Star 1 0.68 --- &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; for two-sided hypotheses, the value tested against lies outside the 95%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. The result shows that the evidence ratio is in favor of the H0 with \\(BF_{01} = 2.22\\). This means that H0 is 2.2 more likely than H1 given the data. 24.3.2 LOO Another way to test different models is to compare them via approximate leave-one-out cross-validation. set.seed(1) df.loo = tibble(x = rnorm(n = 50), y = rnorm(n = 50)) # visualize ggplot(data = df.loo, mapping = aes(x = x, y = y)) + geom_point() # fit the frequentist model fit.lm_loo = lm(formula = y ~ 1 + x, data = df.loo) fit.lm_loo %&gt;% summary() Call: lm(formula = y ~ 1 + x, data = df.loo) Residuals: Min 1Q Median 3Q Max -1.92760 -0.66898 -0.00225 0.48768 2.34858 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.12190 0.13935 0.875 0.386 x -0.04555 0.16807 -0.271 0.788 Residual standard error: 0.9781 on 48 degrees of freedom Multiple R-squared: 0.001528, Adjusted R-squared: -0.01927 F-statistic: 0.07345 on 1 and 48 DF, p-value: 0.7875 # fit and compare bayesian models fit.brm_loo1 = brm(formula = y ~ 1, data = df.loo, seed = 1, file = &quot;cache/brm_loo1&quot;) fit.brm_loo2 = brm(formula = y ~ 1 + x, data = df.loo, seed = 1, file = &quot;cache/brm_loo2&quot;) fit.brm_loo1 = add_criterion(fit.brm_loo1, criterion = &quot;loo&quot;, file = &quot;cache/brm_loo1&quot;) fit.brm_loo2 = add_criterion(fit.brm_loo2, criterion = &quot;loo&quot;, file = &quot;cache/brm_loo2&quot;) loo_compare(fit.brm_loo1, fit.brm_loo2) elpd_diff se_diff fit.brm_loo1 0.0 0.0 fit.brm_loo2 -1.1 0.5 model_weights(fit.brm_loo1, fit.brm_loo2) fit.brm_loo1 fit.brm_loo2 9.999986e-01 1.351561e-06 24.4 Dealing with heteroscedasticity Let’s generate some fake developmental data where the variance in the data is greatest for young children, smaller for older children, and even smaller for adults: # make example reproducible set.seed(1) df.variance = tibble(group = rep(c(&quot;3yo&quot;, &quot;5yo&quot;, &quot;adults&quot;), each = 20), response = rnorm(n = 60, mean = rep(c(0, 5, 8), each = 20), sd = rep(c(3, 1.5, 0.3), each = 20))) 24.4.1 Visualize the data df.variance %&gt;% ggplot(aes(x = group, y = response)) + geom_jitter(height = 0, width = 0.1, alpha = 0.7) 24.4.2 Frequentist analysis 24.4.2.1 Fit the model fit.lm_variance = lm(formula = response ~ 1 + group, data = df.variance) fit.lm_variance %&gt;% summary() Call: lm(formula = response ~ 1 + group, data = df.variance) Residuals: Min 1Q Median 3Q Max -7.2157 -0.3613 0.0200 0.7001 4.2143 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.5716 0.3931 1.454 0.151 group5yo 4.4187 0.5560 7.948 8.4e-11 *** groupadults 7.4701 0.5560 13.436 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.758 on 57 degrees of freedom Multiple R-squared: 0.762, Adjusted R-squared: 0.7537 F-statistic: 91.27 on 2 and 57 DF, p-value: &lt; 2.2e-16 fit.lm_variance %&gt;% glance() # A tibble: 1 × 12 r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0.762 0.754 1.76 91.3 1.70e-18 2 -117. 243. 251. # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; 24.4.2.2 Visualize the model predictions set.seed(1) fit.lm_variance %&gt;% simulate() %&gt;% bind_cols(df.variance) %&gt;% ggplot(aes(x = group, y = sim_1)) + geom_jitter(height = 0, width = 0.1, alpha = 0.7) Notice how the model predicts that the variance is equal for each group. 24.4.3 Bayesian analysis While frequentist models (such as a linear regression) assume equality of variance, Bayesian models afford us with the flexibility of inferring both the parameter estimates of the groups (i.e. the means and differences between the means), as well as the variances. 24.4.3.1 Fit the model We define a multivariate model which tries to fit both the response as well as the variance sigma: fit.brm_variance = brm(formula = bf(response ~ group, sigma ~ group), data = df.variance, file = &quot;cache/brm_variance&quot;, seed = 1) summary(fit.brm_variance) Family: gaussian Links: mu = identity; sigma = log Formula: response ~ group sigma ~ group Data: df.variance (Number of observations: 60) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept -0.01 0.73 -1.41 1.51 1.01 1107 1072 sigma_Intercept 1.15 0.17 0.85 1.51 1.00 1991 1922 group5yo 5.18 0.77 3.60 6.65 1.00 1253 1327 groupadults 7.98 0.74 6.47 9.37 1.01 1110 1079 sigma_group5yo -1.05 0.24 -1.51 -0.57 1.00 2249 2420 sigma_groupadults -2.19 0.24 -2.66 -1.74 1.00 2171 2427 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Notice that sigma is on the log scale. To get the standard deviations, we have to exponentiate the predictors, like so: fit.brm_variance %&gt;% tidy(parameters = &quot;^b_&quot;) %&gt;% filter(str_detect(term, &quot;sigma&quot;)) %&gt;% select(term, estimate) %&gt;% mutate(term = str_remove(term, &quot;b_sigma_&quot;)) %&gt;% pivot_wider(names_from = term, values_from = estimate) %&gt;% clean_names() %&gt;% mutate(across(-intercept, ~ exp(. + intercept))) %&gt;% mutate(intercept = exp(intercept)) Warning in tidy.brmsfit(., parameters = &quot;^b_&quot;): some parameter names contain underscores: term naming may be unreliable! # A tibble: 1 × 3 intercept group5yo groupadults &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3.16 1.10 0.352 24.4.3.2 Visualize the model predictions df.variance %&gt;% expand(group) %&gt;% add_epred_draws(object = fit.brm_variance, dpar = TRUE ) %&gt;% select(group, .row, .draw, posterior = .epred, mu, sigma) %&gt;% pivot_longer(cols = c(mu, sigma), names_to = &quot;index&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value, y = group)) + stat_halfeye() + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + facet_grid(cols = vars(index)) This plot shows what the posterior looks like for both mu (the inferred means), and for sigma (the inferred variances) for the different groups. set.seed(1) df.variance %&gt;% add_predicted_draws(object = fit.brm_variance, ndraws = 1) %&gt;% ggplot(aes(x = group, y = .prediction)) + geom_jitter(height = 0, width = 0.1, alpha = 0.7) 24.5 Zero-one inflated beta binomial model See this blog post. 24.6 Ordinal regression Check out the following two papers: Liddell and Kruschke (2018) Bürkner and Vuorre (2019) Let’s read in some movie ratings: df.movies = read_csv(file = &quot;data/MoviesData.csv&quot;) df.movies = df.movies %&gt;% pivot_longer(cols = n1:n5, names_to = &quot;stars&quot;, values_to = &quot;rating&quot;) %&gt;% mutate(stars = str_remove(stars,&quot;n&quot;), stars = as.numeric(stars)) df.movies = df.movies %&gt;% uncount(weights = rating) %&gt;% mutate(id = as.factor(ID)) %&gt;% filter(ID &lt;= 6) 24.6.1 Ordinal regression (assuming equal variance) 24.6.1.1 Fit the model fit.brm_ordinal = brm(formula = stars ~ 1 + id, family = cumulative(link = &quot;probit&quot;), data = df.movies, file = &quot;cache/brm_ordinal&quot;, seed = 1) summary(fit.brm_ordinal) Family: cumulative Links: mu = probit; disc = identity Formula: stars ~ 1 + id Data: df.movies (Number of observations: 21708) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept[1] -1.22 0.04 -1.30 -1.14 1.00 1483 2215 Intercept[2] -0.90 0.04 -0.98 -0.82 1.00 1448 1966 Intercept[3] -0.44 0.04 -0.52 -0.36 1.00 1419 1853 Intercept[4] 0.33 0.04 0.25 0.41 1.00 1380 1861 id2 0.84 0.06 0.72 0.96 1.00 2045 2455 id3 0.22 0.06 0.11 0.33 1.00 1973 2400 id4 0.33 0.04 0.25 0.42 1.00 1491 1937 id5 0.45 0.05 0.34 0.55 1.00 1734 2681 id6 0.76 0.04 0.68 0.84 1.00 1367 1851 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS disc 1.00 0.00 1.00 1.00 NA NA NA Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). 24.6.1.2 Visualizations 24.6.1.2.1 Model parameters The model infers the thresholds and the means of the Gaussian distributions in latent space. df.params = fit.brm_ordinal %&gt;% parameters(centrality = &quot;mean&quot;) %&gt;% as_tibble() %&gt;% clean_names() %&gt;% select(term = parameter, estimate = mean) ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = ~ dnorm(.), size = 1, color = &quot;black&quot;) + stat_function(fun = ~ dnorm(., mean = df.params %&gt;% filter(str_detect(term, &quot;id2&quot;)) %&gt;% pull(estimate)), size = 1, color = &quot;blue&quot;) + geom_vline(xintercept = df.params %&gt;% filter(str_detect(term, &quot;Intercept&quot;)) %&gt;% pull(estimate)) 24.6.1.2.2 MCMC inference fit.brm_ordinal %&gt;% plot(N = 9, variable = &quot;^b_&quot;, regex = T) fit.brm_ordinal %&gt;% pp_check(ndraws = 20) 24.6.1.2.3 Model predictions conditional_effects(fit.brm_ordinal, effects = &quot;id&quot;, categorical = T) df.model = add_epred_draws(newdata = expand_grid(id = 1:6), object = fit.brm_ordinal, ndraws = 10) df.plot = df.movies %&gt;% count(id, stars) %&gt;% group_by(id) %&gt;% mutate(p = n / sum(n)) %&gt;% mutate(stars = as.factor(stars)) ggplot(data = df.plot, mapping = aes(x = stars, y = p)) + geom_col(color = &quot;black&quot;, fill = &quot;lightblue&quot;) + geom_point(data = df.model, mapping = aes(x = .category, y = .epred), alpha = 0.3, position = position_jitter(width = 0.3)) + facet_wrap(~id, ncol = 6) 24.6.2 Gaussian regression (assuming equal variance) 24.6.2.1 Fit the model fit.brm_metric = brm(formula = stars ~ 1 + id, data = df.movies, file = &quot;cache/brm_metric&quot;, seed = 1) summary(fit.brm_metric) Family: gaussian Links: mu = identity; sigma = identity Formula: stars ~ 1 + id Data: df.movies (Number of observations: 21708) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 3.77 0.04 3.70 3.85 1.00 1100 1471 id2 0.64 0.05 0.54 0.75 1.00 1532 2032 id3 0.20 0.05 0.10 0.30 1.00 1435 2045 id4 0.37 0.04 0.29 0.45 1.00 1146 1448 id5 0.30 0.05 0.20 0.39 1.00 1416 1784 id6 0.72 0.04 0.64 0.80 1.00 1130 1506 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 1.00 0.00 0.99 1.01 1.00 3221 2577 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). 24.6.2.2 Visualizations 24.6.2.2.1 Model predictions # get the predictions for each value of the Likert scale df.model = fit.brm_metric %&gt;% parameters(centrality = &quot;mean&quot;) %&gt;% as_tibble() %&gt;% select(term = Parameter, estimate = Mean) %&gt;% mutate(term = str_remove(term, &quot;b_&quot;)) %&gt;% pivot_wider(names_from = term, values_from = estimate) %&gt;% clean_names() %&gt;% mutate(across(.cols = id2:id6, .fns = ~ . + intercept)) %&gt;% rename_with(.fn = ~ c(str_c(&quot;mu_&quot;, 1:6), &quot;sigma&quot;)) %&gt;% pivot_longer(cols = contains(&quot;mu&quot;), names_to = c(&quot;parameter&quot;, &quot;movie&quot;), names_sep = &quot;_&quot;, values_to = &quot;value&quot;) %&gt;% pivot_wider(names_from = parameter, values_from = value) %&gt;% mutate(data = map2(.x = mu, .y = sigma, .f = ~ tibble(x = 1:5, y = dnorm(x, mean = .x, sd = .y)))) %&gt;% select(movie, data) %&gt;% unnest(c(data)) %&gt;% group_by(movie) %&gt;% mutate(y = y/sum(y)) %&gt;% ungroup() %&gt;% rename(id = movie) # visualize the predictions df.plot = df.movies %&gt;% count(id, stars) %&gt;% group_by(id) %&gt;% mutate(p = n / sum(n)) %&gt;% mutate(stars = as.factor(stars)) ggplot(data = df.plot, mapping = aes(x = stars, y = p)) + geom_col(color = &quot;black&quot;, fill = &quot;lightblue&quot;) + geom_point(data = df.model, mapping = aes(x = x, y = y)) + facet_wrap(~id, ncol = 6) 24.6.3 Oridnal regression (unequal variance) 24.6.3.1 Fit the model fit.brm_ordinal_variance = brm(formula = bf(stars ~ 1 + id) + lf(disc ~ 0 + id, cmc = FALSE), family = cumulative(link = &quot;probit&quot;), data = df.movies, file = &quot;cache/brm_ordinal_variance&quot;, seed = 1) summary(fit.brm_ordinal_variance) Family: cumulative Links: mu = probit; disc = log Formula: stars ~ 1 + id disc ~ 0 + id Data: df.movies (Number of observations: 21708) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept[1] -1.41 0.06 -1.53 -1.29 1.00 1395 2292 Intercept[2] -1.00 0.05 -1.10 -0.91 1.00 1906 2567 Intercept[3] -0.46 0.04 -0.54 -0.38 1.00 2645 2897 Intercept[4] 0.41 0.05 0.32 0.50 1.00 1358 1824 id2 2.70 0.32 2.13 3.38 1.00 1577 2220 id3 0.33 0.07 0.20 0.48 1.00 1795 2246 id4 0.36 0.05 0.26 0.46 1.00 1506 1917 id5 1.64 0.17 1.32 1.98 1.00 1568 2099 id6 0.86 0.06 0.74 0.97 1.00 1065 1919 disc_id2 -1.12 0.10 -1.32 -0.93 1.00 1557 2377 disc_id3 -0.23 0.06 -0.34 -0.12 1.00 1336 2071 disc_id4 -0.01 0.04 -0.10 0.07 1.00 927 1572 disc_id5 -1.09 0.07 -1.23 -0.95 1.00 1249 1966 disc_id6 -0.07 0.04 -0.15 0.01 1.00 856 1528 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). 24.6.3.2 Visualizations 24.6.3.2.1 Model parameters df.params = fit.brm_ordinal_variance %&gt;% tidy(parameters = &quot;^b_&quot;) %&gt;% select(term, estimate) %&gt;% mutate(term = str_remove(term, &quot;b_&quot;)) Warning in tidy.brmsfit(., parameters = &quot;^b_&quot;): some parameter names contain underscores: term naming may be unreliable! ggplot(data = tibble(x = c(-3, 3)), mapping = aes(x = x)) + stat_function(fun = ~ dnorm(.), size = 1, color = &quot;black&quot;) + stat_function(fun = ~ dnorm(., mean = 1, sd = 2), size = 1, color = &quot;blue&quot;) + geom_vline(xintercept = df.params %&gt;% filter(str_detect(term, &quot;Intercept&quot;)) %&gt;% pull(estimate)) 24.6.3.2.2 Model predictions df.model = add_epred_draws(newdata = expand_grid(id = 1:6), object = fit.brm_ordinal_variance, ndraws = 10) df.plot = df.movies %&gt;% count(id, stars) %&gt;% group_by(id) %&gt;% mutate(p = n / sum(n)) %&gt;% mutate(stars = as.factor(stars)) ggplot(data = df.plot, mapping = aes(x = stars, y = p)) + geom_col(color = &quot;black&quot;, fill = &quot;lightblue&quot;) + geom_point(data = df.model, mapping = aes(x = .category, y = .epred), alpha = 0.3, position = position_jitter(width = 0.3)) + facet_wrap(~id, ncol = 6) 24.6.4 Gaussian regression (unequal variance) 24.6.4.1 Fit the model fit.brm_metric_variance = brm(formula = bf(stars ~ 1 + id, sigma ~ 1 + id), data = df.movies, file = &quot;cache/brm_metric_variance&quot;, seed = 1) summary(fit.brm_metric_variance) Family: gaussian Links: mu = identity; sigma = log Formula: stars ~ 1 + id sigma ~ 1 + id Data: df.movies (Number of observations: 21708) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 3.77 0.05 3.68 3.86 1.00 1562 1853 sigma_Intercept 0.20 0.03 0.15 0.26 1.00 1194 1613 id2 0.64 0.07 0.51 0.77 1.00 2058 2521 id3 0.20 0.06 0.08 0.33 1.00 2029 2337 id4 0.37 0.05 0.27 0.47 1.00 1660 2146 id5 0.30 0.06 0.17 0.42 1.00 2033 2370 id6 0.72 0.05 0.63 0.81 1.00 1601 1961 sigma_id2 0.02 0.04 -0.05 0.09 1.00 1727 2141 sigma_id3 0.03 0.04 -0.04 0.10 1.00 1436 1982 sigma_id4 -0.14 0.03 -0.19 -0.08 1.00 1266 1581 sigma_id5 0.20 0.03 0.14 0.27 1.00 1505 2072 sigma_id6 -0.35 0.03 -0.40 -0.29 1.00 1210 1576 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). 24.6.4.2 Visualizations 24.6.4.2.1 Model predictions df.model = fit.brm_metric_variance %&gt;% tidy(parameters = &quot;^b_&quot;) %&gt;% select(term, estimate) %&gt;% mutate(term = str_remove(term, &quot;b_&quot;)) %&gt;% pivot_wider(names_from = term, values_from = estimate) %&gt;% clean_names() %&gt;% mutate(across(.cols = c(id2:id6), .fns = ~ . + intercept)) %&gt;% mutate(across(.cols = contains(&quot;sigma&quot;), .fns = ~ 1/exp(.))) %&gt;% mutate(across(.cols = c(sigma_id2:sigma_id5), .fns = ~ . + sigma_intercept)) %&gt;% set_names(c(&quot;mu_1&quot;, &quot;sigma_1&quot;, str_c(&quot;mu_&quot;, 2:6), str_c(&quot;sigma_&quot;, 2:6))) %&gt;% pivot_longer(cols = everything(), names_to = c(&quot;parameter&quot;, &quot;movie&quot;), names_sep = &quot;_&quot;, values_to = &quot;value&quot;) %&gt;% pivot_wider(names_from = parameter, values_from = value) %&gt;% mutate(data = map2(.x = mu, .y = sigma, .f = ~ tibble(x = 1:5, y = dnorm(x, mean = .x, sd = .y)))) %&gt;% select(movie, data) %&gt;% unnest(c(data)) %&gt;% group_by(movie) %&gt;% mutate(y = y/sum(y)) %&gt;% ungroup() %&gt;% rename(id = movie) Warning in tidy.brmsfit(., parameters = &quot;^b_&quot;): some parameter names contain underscores: term naming may be unreliable! df.plot = df.movies %&gt;% count(id, stars) %&gt;% group_by(id) %&gt;% mutate(p = n / sum(n)) %&gt;% mutate(stars = as.factor(stars)) ggplot(data = df.plot, mapping = aes(x = stars, y = p)) + geom_col(color = &quot;black&quot;, fill = &quot;lightblue&quot;) + geom_point(data = df.model, mapping = aes(x = x, y = y)) + facet_wrap(~id, ncol = 6) 24.6.5 Model comparison # ordinal regression with equal variance fit.brm_ordinal = add_criterion(fit.brm_ordinal, criterion = &quot;loo&quot;, file = &quot;cache/brm_ordinal&quot;) # Gaussian regression with equal variance fit.brm_ordinal_variance = add_criterion(fit.brm_ordinal_variance, criterion = &quot;loo&quot;, file = &quot;cache/brm_ordinal_variance&quot;) loo_compare(fit.brm_ordinal, fit.brm_ordinal_variance) elpd_diff se_diff fit.brm_ordinal_variance 0.0 0.0 fit.brm_ordinal -340.2 27.0 24.7 Additional resources Tutorial on visualizing brms posteriors with tidybayes Hypothetical outcome plots Visual MCMC diagnostics Visualiztion of different MCMC algorithms For additional resources, I highly recommend the brms and tidyverse implementations of the Statistical rethinking book (McElreath 2020), as well as of the Doing Bayesian Data analysis book (Kruschke 2014), by Solomon Kurz (Kurz 2020, 2022). 24.8 Session info Information about this R session including which version of R was used, and what packages were loaded. sessionInfo() R version 4.1.2 (2021-11-01) Platform: x86_64-apple-darwin17.0 (64-bit) Running under: macOS Big Sur 10.16 Matrix products: default BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib locale: [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 attached base packages: [1] stats graphics grDevices utils datasets methods base other attached packages: [1] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.9 [4] purrr_0.3.4 readr_2.1.2 tidyr_1.2.0 [7] tibble_3.1.7 tidyverse_1.3.1 scales_1.2.0 [10] ggrepel_0.9.1 rstanarm_2.21.3 transformr_0.1.3 [13] parameters_0.17.0 gganimate_1.0.7 titanic_0.1.0 [16] ggeffects_1.1.2 emmeans_1.7.3 car_3.0-13 [19] carData_3.0-5 afex_1.1-1 lme4_1.1-29 [22] Matrix_1.4-1 modelr_0.1.8 bayesplot_1.9.0 [25] broom.mixed_0.2.9.4 GGally_2.1.2 ggplot2_3.3.6 [28] patchwork_1.1.1 brms_2.17.0 Rcpp_1.0.8.3 [31] tidybayes_3.0.2 janitor_2.1.0 kableExtra_1.3.4 [34] knitr_1.39 loaded via a namespace (and not attached): [1] utf8_1.2.2 tidyselect_1.1.2 htmlwidgets_1.5.4 [4] grid_4.1.2 lpSolve_5.6.15 munsell_0.5.0 [7] codetools_0.2-18 units_0.8-0 DT_0.22 [10] future_1.25.0 gifski_1.6.6-1 miniUI_0.1.1.1 [13] withr_2.5.0 Brobdingnag_1.2-7 colorspace_2.0-3 [16] highr_0.9 rstudioapi_0.13 stats4_4.1.2 [19] listenv_0.8.0 labeling_0.4.2 rstan_2.21.5 [22] bit64_4.0.5 farver_2.1.0 datawizard_0.4.0 [25] bridgesampling_1.1-2 coda_0.19-4 parallelly_1.31.1 [28] vctrs_0.4.1 generics_0.1.2 TH.data_1.1-1 [31] xfun_0.30 R6_2.5.1 markdown_1.1 [34] reshape_0.8.9 assertthat_0.2.1 vroom_1.5.7 [37] promises_1.2.0.1 multcomp_1.4-19 gtable_0.3.0 [40] globals_0.14.0 processx_3.5.3 sandwich_3.0-1 [43] rlang_1.0.2 systemfonts_1.0.4 splines_4.1.2 [46] broom_0.8.0 checkmate_2.1.0 inline_0.3.19 [49] yaml_2.3.5 reshape2_1.4.4 abind_1.4-5 [52] threejs_0.3.3 crosstalk_1.2.0 backports_1.4.1 [55] httpuv_1.6.5 tensorA_0.36.2 tools_4.1.2 [58] bookdown_0.26 ellipsis_0.3.2 jquerylib_0.1.4 [61] posterior_1.2.1 RColorBrewer_1.1-3 proxy_0.4-26 [64] ggridges_0.5.3 plyr_1.8.7 base64enc_0.1-3 [67] progress_1.2.2 classInt_0.4-3 ps_1.7.0 [70] prettyunits_1.1.1 zoo_1.8-10 haven_2.5.0 [73] fs_1.5.2 furrr_0.3.0 magrittr_2.0.3 [76] ggdist_3.1.1 lmerTest_3.1-3 reprex_2.0.1 [79] colourpicker_1.1.1 mvtnorm_1.1-3 matrixStats_0.62.0 [82] hms_1.1.1 shinyjs_2.1.0 mime_0.12 [85] evaluate_0.15 arrayhelpers_1.1-0 xtable_1.8-4 [88] shinystan_2.6.0 readxl_1.4.0 gridExtra_2.3 [91] rstantools_2.2.0 compiler_4.1.2 KernSmooth_2.23-20 [94] crayon_1.5.1 minqa_1.2.4 StanHeaders_2.21.0-7 [97] htmltools_0.5.2 tzdb_0.3.0 later_1.3.0 [100] RcppParallel_5.1.5 lubridate_1.8.0 DBI_1.1.2 [103] tweenr_1.0.2 dbplyr_2.1.1 MASS_7.3-57 [106] sf_1.0-7 boot_1.3-28 cli_3.3.0 [109] parallel_4.1.2 insight_0.17.0 igraph_1.3.1 [112] pkgconfig_2.0.3 numDeriv_2016.8-1.1 xml2_1.3.3 [115] svUnit_1.0.6 dygraphs_1.1.1.6 svglite_2.1.0 [118] bslib_0.3.1 webshot_0.5.3 estimability_1.3 [121] rvest_1.0.2 snakecase_0.11.0 distributional_0.3.0 [124] callr_3.7.0 digest_0.6.29 cellranger_1.1.0 [127] rmarkdown_2.14 shiny_1.7.1 gtools_3.9.2 [130] nloptr_2.0.0 lifecycle_1.0.1 nlme_3.1-157 [133] jsonlite_1.8.0 viridisLite_0.4.0 fansi_1.0.3 [136] pillar_1.7.0 lattice_0.20-45 loo_2.5.1 [139] fastmap_1.1.0 httr_1.4.3 pkgbuild_1.3.1 [142] survival_3.3-1 glue_1.6.2 xts_0.12.1 [145] bayestestR_0.12.1 shinythemes_1.2.0 bit_4.0.4 [148] class_7.3-20 stringi_1.7.6 sass_0.4.1 [151] e1071_1.7-9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
